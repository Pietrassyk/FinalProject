{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========SETUP========\n",
      "Connecting to mysql\n",
      "successfully imported module\n",
      "Connection to mysql successfull. with curser MySQLCursor: (Nothing executed yet)\n",
      "Executed Querry\n",
      "No Jobs available\n"
     ]
    }
   ],
   "source": [
    "#IMPORTS\n",
    "########\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import tpclean.tpclean as tp\n",
    "\n",
    "#custom imports\n",
    "from Scripts.config import role, bucket_name, prefix, bucket_path, sub_path\n",
    "from Scripts.nlp_functions import generate_wordcloud , get_summary\n",
    "\n",
    "#establish connection\n",
    "from Scripts.config import host, db\n",
    "from Private.private import user , password\n",
    "\n",
    "#NLTK\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#Imports for Bigrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from Scripts.nlp_functions import bigram_to_single_word\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#API Imports\n",
    "from Private.private import aylien_app_id,aylien_API_KEY\n",
    "\n",
    "print(\"========SETUP========\")\n",
    "#load credentials\n",
    "conn_kwargs = {\"host\":host, \n",
    "               \"user\":user, \n",
    "               \"password\":password}\n",
    "conn = tp.sql_connect(db,db_type=\"mysql\",**conn_kwargs)\n",
    "#conn = mysql.connector.Connect(database = db, **conn_kwargs)\n",
    "c = conn.cursor()\n",
    "\n",
    "#get all the files that dont have frequencies yet\n",
    "words_df = tp.sql(\"\"\"SELECT origin\n",
    "FROM content\n",
    "GROUP BY origin\n",
    "HAVING COUNT(freq) = 0\"\"\")\n",
    "\n",
    "i=0\n",
    "try:\n",
    "    job_list = list(words_df.iloc[:,0])\n",
    "    print(\"========SETUP COMPLETE========\")\n",
    "except:\n",
    "    print(\"No Jobs available\")\n",
    "    conn.close()\n",
    "    exit()\n",
    "\n",
    "print(\"========Starting JOBs========\")\n",
    "\n",
    "for file in job_list:\n",
    "    #VERBOSE\n",
    "    i += 1\n",
    "    print(f\"Starting Job {i}/{len(job_list)}\")\n",
    "    print(f\"Current File: {file}\")\n",
    "    \n",
    "    #get full text\n",
    "    text_df = tp.sql(f\"\"\"SELECT full_text FROM conversations WHERE filename = \"{file}\" \"\"\")\n",
    "    text = text_df[\"full_text\"][0]\n",
    "    \n",
    "    #get the words from the content table\n",
    "    file_df = tp.sql(f\"\"\"SELECT pos_in_conv,\n",
    "                    LOWER(content) as content\n",
    "                    FROM content\n",
    "                    WHERE type = \"pronunciation\" and origin = \"{file}\" \"\"\")\n",
    "    \n",
    "    ##REMOCE STOPWORDS AND LEMMATISE\n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #define function for stopping and lemmatizing and apply it to the DataFrame\n",
    "    stoplem = lambda x: np.NaN if x in stopword_list else lemmatizer.lemmatize(x)\n",
    "    file_df[\"lemmatized\"] = file_df[\"content\"].apply(stoplem)\n",
    "    \n",
    "    #create freqency dict\n",
    "    fDist_lemm = FreqDist(file_df[\"lemmatized\"].dropna(),)\n",
    "    \n",
    "    #append frequencies to lemmatized words\n",
    "    freqs = pd.DataFrame.from_dict(fDist_lemm,orient = \"index\", columns = [\"freq\"])\n",
    "    file_df = pd.merge(file_df,freqs, how = \"left\", left_on = \"lemmatized\", right_index = True)\n",
    "    \n",
    "    ## CREATE BIGRAMS\n",
    "    finder = BigramCollocationFinder.from_words(file_df[\"lemmatized\"].dropna())\n",
    "    finder.nbest(BigramAssocMeasures.likelihood_ratio, 10)\n",
    "    bigrams_fd = finder.ngram_fd\n",
    "    \n",
    "    #visualize in a wordcloud\n",
    "    testfd = bigram_to_single_word(bigrams_fd)\n",
    "    generate_wordcloud(testfd)\n",
    "    \n",
    "    #SENTIMENT ANALYSIS\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sc = lambda x: sia.polarity_scores(x)[\"compound\"]\n",
    "    file_df[\"sentiment_score\"] = file_df[\"lemmatized\"].dropna().apply(sc)\n",
    "    file_df[\"weight\"] = file_df[\"freq\"]*file_df[\"sentiment_score\"]\n",
    "    \n",
    "    #update DB\n",
    "    updates = file_df.dropna()\n",
    "    for i in range(len(updates)):\n",
    "        row = updates.iloc[i]\n",
    "        \n",
    "        #build UPDATE querry\n",
    "        out =[f'{key} = \"{row[key]}\"' for key in row.keys()[1:]]\n",
    "        set_ = \", \".join(out)\n",
    "        querry = f\"\"\"UPDATE content\n",
    "                     SET {set_}\n",
    "                     WHERE pos_in_conv = {row['pos_in_conv']}\n",
    "                     AND origin = \"{file}\"\n",
    "                 \"\"\"\n",
    "        c.execute(querry)\n",
    "    conn.commit()\n",
    "    print(\"Inserted frequency dict in content\")\n",
    "    \n",
    "    #GET SUMMARY\n",
    "    summary_text = get_summary(file, text, aylien_app_id, aylien_API_KEY)\n",
    "    \n",
    "    c.execute(f\"\"\"UPDATE conversations\n",
    "            SET summary = \"{summary_text}\"\n",
    "            WHERE filename = \"{file}\" \"\"\")\n",
    "    conn.commit()\n",
    "conn.close()\n",
    "print(\"========ALL JOBS FINISHED========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
